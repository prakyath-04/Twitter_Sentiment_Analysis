{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP90duWXc528"
   },
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNl_g8-dc9KN"
   },
   "source": [
    "### Benchmarking different Tokenizers and Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODScwQ6DdE38"
   },
   "source": [
    "### Importing python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "amBDZOoG6tqB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import * \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "import torch\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm \n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel,BertConfig,BertForMaskedLM, BertForSequenceClassification\n",
    "import random\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7-1AHlmdQQK"
   },
   "source": [
    "### Loading Data\n",
    "We used Sentiment140 dataset from stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BykOWaXNAWy-",
    "outputId": "7103872f-7783-44d2-ac9e-9e5d75598953"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>account_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369726</th>\n",
       "      <td>0</td>\n",
       "      <td>2049957854</td>\n",
       "      <td>Fri Jun 05 17:48:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>hepkitten</td>\n",
       "      <td>@ryanlrussell its graduation and wedding seaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582096</th>\n",
       "      <td>0</td>\n",
       "      <td>2214498731</td>\n",
       "      <td>Wed Jun 17 16:49:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ICJenny</td>\n",
       "      <td>i missed adam lambert's live chat on comcast b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373836</th>\n",
       "      <td>4</td>\n",
       "      <td>2051364957</td>\n",
       "      <td>Fri Jun 05 21:48:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>debbbbbie</td>\n",
       "      <td>i really want @ddlovato 's people to email me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346806</th>\n",
       "      <td>4</td>\n",
       "      <td>2044384786</td>\n",
       "      <td>Fri Jun 05 09:16:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>geonz</td>\n",
       "      <td>Just remembered FONGING.  Must bring materials...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047292</th>\n",
       "      <td>4</td>\n",
       "      <td>1957752145</td>\n",
       "      <td>Fri May 29 01:29:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>darrenporter</td>\n",
       "      <td>@NikkiPilkington cool...... thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "369726           0  2049957854  Fri Jun 05 17:48:24 PDT 2009  NO_QUERY   \n",
       "582096           0  2214498731  Wed Jun 17 16:49:48 PDT 2009  NO_QUERY   \n",
       "1373836          4  2051364957  Fri Jun 05 21:48:38 PDT 2009  NO_QUERY   \n",
       "1346806          4  2044384786  Fri Jun 05 09:16:47 PDT 2009  NO_QUERY   \n",
       "1047292          4  1957752145  Fri May 29 01:29:54 PDT 2009  NO_QUERY   \n",
       "\n",
       "           account_id                                              tweet  \n",
       "369726      hepkitten  @ryanlrussell its graduation and wedding seaso...  \n",
       "582096        ICJenny  i missed adam lambert's live chat on comcast b...  \n",
       "1373836     debbbbbie  i really want @ddlovato 's people to email me ...  \n",
       "1346806         geonz  Just remembered FONGING.  Must bring materials...  \n",
       "1047292  darrenporter                @NikkiPilkington cool...... thanks   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update path according to your directory\n",
    "data1 = pd.read_csv('/scratch/pm3140/training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1',header=None,names=['sentiment','id','date','query','account_id','tweet'])\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "data1 = data1.sample(frac = 1) ## shuffling the data\n",
    "data_copy = data1 ## making a copy of the data\n",
    "data_copy = data1[:1000]  ## Uncomment for selecting a subset of the dataset\n",
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dh4_QD_kdmxH"
   },
   "source": [
    "### Shuffling, Relabeling data. \n",
    "0 - negative tweet\n",
    "1 - positive tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "25nUsAN2GTI3",
    "outputId": "560b35d8-23c2-4257-b064-89bcd06772df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>account_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369726</th>\n",
       "      <td>0</td>\n",
       "      <td>2049957854</td>\n",
       "      <td>Fri Jun 05 17:48:24 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>hepkitten</td>\n",
       "      <td>@ryanlrussell its graduation and wedding seaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582096</th>\n",
       "      <td>0</td>\n",
       "      <td>2214498731</td>\n",
       "      <td>Wed Jun 17 16:49:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ICJenny</td>\n",
       "      <td>i missed adam lambert's live chat on comcast b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373836</th>\n",
       "      <td>1</td>\n",
       "      <td>2051364957</td>\n",
       "      <td>Fri Jun 05 21:48:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>debbbbbie</td>\n",
       "      <td>i really want @ddlovato 's people to email me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346806</th>\n",
       "      <td>1</td>\n",
       "      <td>2044384786</td>\n",
       "      <td>Fri Jun 05 09:16:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>geonz</td>\n",
       "      <td>Just remembered FONGING.  Must bring materials...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047292</th>\n",
       "      <td>1</td>\n",
       "      <td>1957752145</td>\n",
       "      <td>Fri May 29 01:29:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>darrenporter</td>\n",
       "      <td>@NikkiPilkington cool...... thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date     query  \\\n",
       "369726           0  2049957854  Fri Jun 05 17:48:24 PDT 2009  NO_QUERY   \n",
       "582096           0  2214498731  Wed Jun 17 16:49:48 PDT 2009  NO_QUERY   \n",
       "1373836          1  2051364957  Fri Jun 05 21:48:38 PDT 2009  NO_QUERY   \n",
       "1346806          1  2044384786  Fri Jun 05 09:16:47 PDT 2009  NO_QUERY   \n",
       "1047292          1  1957752145  Fri May 29 01:29:54 PDT 2009  NO_QUERY   \n",
       "\n",
       "           account_id                                              tweet  \n",
       "369726      hepkitten  @ryanlrussell its graduation and wedding seaso...  \n",
       "582096        ICJenny  i missed adam lambert's live chat on comcast b...  \n",
       "1373836     debbbbbie  i really want @ddlovato 's people to email me ...  \n",
       "1346806         geonz  Just remembered FONGING.  Must bring materials...  \n",
       "1047292  darrenporter                @NikkiPilkington cool...... thanks   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy['sentiment'].replace({4: 1}, inplace=True) ## positive tweets are labeled 4, mapping them to 1\n",
    "data_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1a8Tv9Sd3Xz"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to remove unimportant patterns in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rAupuA1SUTOD"
   },
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DmqYw7BUX-X",
    "outputId": "e0575e2d-a9dd-4e53-df56-ac4667f93e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 215 ms, sys: 85.1 ms, total: 300 ms\n",
      "Wall time: 363 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#Removing Twitter Handles\n",
    "data_copy['clean_tweet'] = np.vectorize(remove_pattern)(data_copy['tweet'], \"@[\\w]*\") \n",
    "#Removing Punctuations, Numbers, and Special Characters\n",
    "data_copy.clean_tweet = data_copy.clean_tweet.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# Removing Short/abbrevated Words\n",
    "data_copy.clean_tweet = data_copy.clean_tweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "FHMZEqieV58C",
    "outputId": "c31db037-052c-4b90-d261-f90a3ab51c23"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369726</th>\n",
       "      <td>0</td>\n",
       "      <td>2049957854</td>\n",
       "      <td>@ryanlrussell its graduation and wedding seaso...</td>\n",
       "      <td>graduation wedding season work week runs thru ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582096</th>\n",
       "      <td>0</td>\n",
       "      <td>2214498731</td>\n",
       "      <td>i missed adam lambert's live chat on comcast b...</td>\n",
       "      <td>missed adam lambert live chat comcast because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373836</th>\n",
       "      <td>1</td>\n",
       "      <td>2051364957</td>\n",
       "      <td>i really want @ddlovato 's people to email me ...</td>\n",
       "      <td>really want people email back cause being musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346806</th>\n",
       "      <td>1</td>\n",
       "      <td>2044384786</td>\n",
       "      <td>Just remembered FONGING.  Must bring materials...</td>\n",
       "      <td>Just remembered FONGING Must bring materials G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047292</th>\n",
       "      <td>1</td>\n",
       "      <td>1957752145</td>\n",
       "      <td>@NikkiPilkington cool...... thanks</td>\n",
       "      <td>cool thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385859</th>\n",
       "      <td>0</td>\n",
       "      <td>2053684801</td>\n",
       "      <td>@Nanette1 I know I was gonna stop at duncan do...</td>\n",
       "      <td>know gonna stop duncan donuts cause never them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425895</th>\n",
       "      <td>1</td>\n",
       "      <td>2059132991</td>\n",
       "      <td>Just back from an excellent concert from the G...</td>\n",
       "      <td>Just back from excellent concert from Gardiner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197511</th>\n",
       "      <td>1</td>\n",
       "      <td>1984985574</td>\n",
       "      <td>Once again Minia put together a meal worthy of...</td>\n",
       "      <td>Once again Minia together meal worthy royalty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399163</th>\n",
       "      <td>0</td>\n",
       "      <td>2057079950</td>\n",
       "      <td>@DanaCortez  Hope you get to hug him and kiss ...</td>\n",
       "      <td>Hope kiss soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902104</th>\n",
       "      <td>1</td>\n",
       "      <td>1694365323</td>\n",
       "      <td>@FaithfulChosen I'm fine thx. Had 3 days off a...</td>\n",
       "      <td>fine days took advantage them tired Waiting pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id  \\\n",
       "369726           0  2049957854   \n",
       "582096           0  2214498731   \n",
       "1373836          1  2051364957   \n",
       "1346806          1  2044384786   \n",
       "1047292          1  1957752145   \n",
       "385859           0  2053684801   \n",
       "1425895          1  2059132991   \n",
       "1197511          1  1984985574   \n",
       "399163           0  2057079950   \n",
       "902104           1  1694365323   \n",
       "\n",
       "                                                     tweet  \\\n",
       "369726   @ryanlrussell its graduation and wedding seaso...   \n",
       "582096   i missed adam lambert's live chat on comcast b...   \n",
       "1373836  i really want @ddlovato 's people to email me ...   \n",
       "1346806  Just remembered FONGING.  Must bring materials...   \n",
       "1047292                @NikkiPilkington cool...... thanks    \n",
       "385859   @Nanette1 I know I was gonna stop at duncan do...   \n",
       "1425895  Just back from an excellent concert from the G...   \n",
       "1197511  Once again Minia put together a meal worthy of...   \n",
       "399163   @DanaCortez  Hope you get to hug him and kiss ...   \n",
       "902104   @FaithfulChosen I'm fine thx. Had 3 days off a...   \n",
       "\n",
       "                                               clean_tweet  \n",
       "369726   graduation wedding season work week runs thru ...  \n",
       "582096   missed adam lambert live chat comcast because ...  \n",
       "1373836  really want people email back cause being musi...  \n",
       "1346806  Just remembered FONGING Must bring materials G...  \n",
       "1047292                                        cool thanks  \n",
       "385859   know gonna stop duncan donuts cause never them...  \n",
       "1425895  Just back from excellent concert from Gardiner...  \n",
       "1197511  Once again Minia together meal worthy royalty ...  \n",
       "399163                                      Hope kiss soon  \n",
       "902104   fine days took advantage them tired Waiting pi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## removing columns not required for classification \n",
    "data_copy = data_copy.drop(['query','date','account_id'],axis = 1) \n",
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVWYdgTHeeEB"
   },
   "source": [
    "#### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWUyNjzQVm_v",
    "outputId": "1aeb5c30-ebc2-4c02-b515-8d94b4f8db55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 1.85 ms, total: 132 ms\n",
      "Wall time: 131 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## converting tweets into tokens\n",
    "tokenized_tweet = data_copy.clean_tweet.apply(lambda x: x.split())\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "##The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the \n",
    "## commoner morphological and inflexional endings from words in English\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()\n",
    "\n",
    "## reforming tweet from tokens\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet.iloc[i] = ' '.join(tokenized_tweet.iloc[i])    \n",
    "# data_copy['clean_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "FEVT66i9VzVc",
    "outputId": "37239665-8277-42f7-efb0-e67cef7e663f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>369726</th>\n",
       "      <td>0</td>\n",
       "      <td>2049957854</td>\n",
       "      <td>@ryanlrussell its graduation and wedding seaso...</td>\n",
       "      <td>graduation wedding season work week runs thru ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582096</th>\n",
       "      <td>0</td>\n",
       "      <td>2214498731</td>\n",
       "      <td>i missed adam lambert's live chat on comcast b...</td>\n",
       "      <td>missed adam lambert live chat comcast because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373836</th>\n",
       "      <td>1</td>\n",
       "      <td>2051364957</td>\n",
       "      <td>i really want @ddlovato 's people to email me ...</td>\n",
       "      <td>really want people email back cause being musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346806</th>\n",
       "      <td>1</td>\n",
       "      <td>2044384786</td>\n",
       "      <td>Just remembered FONGING.  Must bring materials...</td>\n",
       "      <td>Just remembered FONGING Must bring materials G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047292</th>\n",
       "      <td>1</td>\n",
       "      <td>1957752145</td>\n",
       "      <td>@NikkiPilkington cool...... thanks</td>\n",
       "      <td>cool thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385859</th>\n",
       "      <td>0</td>\n",
       "      <td>2053684801</td>\n",
       "      <td>@Nanette1 I know I was gonna stop at duncan do...</td>\n",
       "      <td>know gonna stop duncan donuts cause never them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425895</th>\n",
       "      <td>1</td>\n",
       "      <td>2059132991</td>\n",
       "      <td>Just back from an excellent concert from the G...</td>\n",
       "      <td>Just back from excellent concert from Gardiner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197511</th>\n",
       "      <td>1</td>\n",
       "      <td>1984985574</td>\n",
       "      <td>Once again Minia put together a meal worthy of...</td>\n",
       "      <td>Once again Minia together meal worthy royalty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399163</th>\n",
       "      <td>0</td>\n",
       "      <td>2057079950</td>\n",
       "      <td>@DanaCortez  Hope you get to hug him and kiss ...</td>\n",
       "      <td>Hope kiss soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902104</th>\n",
       "      <td>1</td>\n",
       "      <td>1694365323</td>\n",
       "      <td>@FaithfulChosen I'm fine thx. Had 3 days off a...</td>\n",
       "      <td>fine days took advantage them tired Waiting pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id  \\\n",
       "369726           0  2049957854   \n",
       "582096           0  2214498731   \n",
       "1373836          1  2051364957   \n",
       "1346806          1  2044384786   \n",
       "1047292          1  1957752145   \n",
       "385859           0  2053684801   \n",
       "1425895          1  2059132991   \n",
       "1197511          1  1984985574   \n",
       "399163           0  2057079950   \n",
       "902104           1  1694365323   \n",
       "\n",
       "                                                     tweet  \\\n",
       "369726   @ryanlrussell its graduation and wedding seaso...   \n",
       "582096   i missed adam lambert's live chat on comcast b...   \n",
       "1373836  i really want @ddlovato 's people to email me ...   \n",
       "1346806  Just remembered FONGING.  Must bring materials...   \n",
       "1047292                @NikkiPilkington cool...... thanks    \n",
       "385859   @Nanette1 I know I was gonna stop at duncan do...   \n",
       "1425895  Just back from an excellent concert from the G...   \n",
       "1197511  Once again Minia put together a meal worthy of...   \n",
       "399163   @DanaCortez  Hope you get to hug him and kiss ...   \n",
       "902104   @FaithfulChosen I'm fine thx. Had 3 days off a...   \n",
       "\n",
       "                                               clean_tweet  \n",
       "369726   graduation wedding season work week runs thru ...  \n",
       "582096   missed adam lambert live chat comcast because ...  \n",
       "1373836  really want people email back cause being musi...  \n",
       "1346806  Just remembered FONGING Must bring materials G...  \n",
       "1047292                                        cool thanks  \n",
       "385859   know gonna stop duncan donuts cause never them...  \n",
       "1425895  Just back from excellent concert from Gardiner...  \n",
       "1197511  Once again Minia together meal worthy royalty ...  \n",
       "399163                                      Hope kiss soon  \n",
       "902104   fine days took advantage them tired Waiting pi...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um59IinygPAh"
   },
   "source": [
    "#### Checking if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k92GfC-ygTyQ",
    "outputId": "931f20f4-c578-4335-bf67-854a9bc5830e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "## use cuda if available\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFOoBU1wkz8R"
   },
   "source": [
    "#### Splitting data into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yQhXQhQ1kyRZ"
   },
   "outputs": [],
   "source": [
    "### data(tweets) and labels\n",
    "x = data_copy.clean_tweet.values\n",
    "y = data_copy.sentiment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to fit different ML algorithms on the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x_train,x_val,y_train,y_val):\n",
    "    # Logistic Regression\n",
    "    start = time.time()\n",
    "    lreg = LogisticRegression(solver='lbfgs') \n",
    "    lreg.fit(x_train, y_train) \n",
    "    stop = time.time()\n",
    "    print('time taken:',stop-start)\n",
    "    prediction = lreg.predict_proba(x_val) # predicting on the validation set \n",
    "    preds = prediction[:,1] >= 0.4 # if prediction is greater than or equal to 0.4 than 1 else 0 \n",
    "    preds = preds.astype(int)\n",
    "\n",
    "    ## Logistic Regression Accuracy\n",
    "    print('Logistic regression Accuracy and F1 Score:')\n",
    "    print(accuracy_score(preds,y_val)) # calculating accuracy for the validation set\n",
    "    print(f1_score(preds,y_val)) # calculating f1 score for the validation set\n",
    "\n",
    "    ## SVM\n",
    "    start = time.time()\n",
    "    clf_svm = SVC(kernel='rbf', C=1, probability=True,max_iter=10000)\n",
    "    clf_svm.fit(x_train, y_train) \n",
    "    stop = time.time()\n",
    "    print('time taken:',stop-start)\n",
    "    prediction = clf_svm.predict_proba(x_val) # predicting on the validation set \n",
    "    preds = prediction[:,1] >= 0.4 # if prediction is greater than or equal to 0.4 than 1 else 0 \n",
    "    preds = preds.astype(int)\n",
    "\n",
    "    ## SVM accuracy\n",
    "    print('SVM Accuracy and F1 Score:')\n",
    "    print(accuracy_score(preds,y_val)) # calculating accuracy for the validation set\n",
    "    print(f1_score(preds,y_val)) # calculating f1 score for the validation set\n",
    "\n",
    "\n",
    "    # ### Random Forrest\n",
    "    start = time.time()\n",
    "    clf_rf= RandomForestClassifier(n_estimators=100)\n",
    "    clf_rf.fit(x_train, y_train) \n",
    "    stop = time.time()\n",
    "    print('time taken:',stop-start)\n",
    "    prediction = clf_rf.predict_proba(x_val) # predicting on the validation set \n",
    "    preds = prediction[:,1] >= 0.4 # if prediction is greater than or equal to 0.4 than 1 else 0 \n",
    "    preds = preds.astype(int)\n",
    "\n",
    "    ## Random Forrest accuracy\n",
    "    print('Random Forrest Accuracy and F1 Score:')\n",
    "    print(accuracy_score(preds,y_val)) # calculating accuracy for the validation set\n",
    "    print(f1_score(preds,y_val)) # calculating f1 score for the validation set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB0VcaxTf2so"
   },
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj-bVPwyf5rG"
   },
   "source": [
    "### TF-IDF Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vd7TXr7nIm5R",
    "outputId": "7261965f-a17e-4454-85f5-1ef418c44043"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 ms, sys: 760 µs, total: 19.2 ms\n",
      "Wall time: 17.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y,shuffle=True,test_size=0.2)\n",
    "tf_idf = TfidfVectorizer(smooth_idf=False)\n",
    "x_train = tf_idf.fit_transform(x_train)\n",
    "x_val = tf_idf.transform(x_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.41712021827697754\n",
      "Logistic regression Accuracy and F1 Score:\n",
      "0.61\n",
      "0.7022900763358778\n",
      "time taken: 0.2205650806427002\n",
      "SVM Accuracy and F1 Score:\n",
      "0.625\n",
      "0.6781115879828327\n",
      "time taken: 0.22633790969848633\n",
      "Random Forrest Accuracy and F1 Score:\n",
      "0.565\n",
      "0.5876777251184834\n"
     ]
    }
   ],
   "source": [
    "classify(x_train,x_val,y_train,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US5LK_M1f98d"
   },
   "source": [
    "### Bag of words tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fzhsOc5aIRp",
    "outputId": "f676dc50-9fbd-4da2-f221-440c3a7c05eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 257 µs, total: 15.9 ms\n",
      "Wall time: 14.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "x_bow = bow_vectorizer.fit_transform(x)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_bow,y,shuffle=True,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.019404172897338867\n",
      "Logistic regression Accuracy and F1 Score:\n",
      "0.655\n",
      "0.7136929460580913\n",
      "time taken: 0.13352560997009277\n",
      "SVM Accuracy and F1 Score:\n",
      "0.585\n",
      "0.6693227091633466\n",
      "time taken: 0.21800565719604492\n",
      "Random Forrest Accuracy and F1 Score:\n",
      "0.595\n",
      "0.6610878661087866\n"
     ]
    }
   ],
   "source": [
    "classify(x_train,x_val,y_train,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ4zadSegBUw"
   },
   "source": [
    "### Word2Vec Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "id": "ubwKnhsCTlV8",
    "outputId": "925213f5-8d34-4f49-8ce8-7b8eac04edb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.06 s, sys: 382 ms, total: 4.45 s\n",
      "Wall time: 4.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(401883, 694700)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# tokenizing clean tweet\n",
    "tokenized_tweet = data_copy['clean_tweet'].apply(lambda x: x.split()) \n",
    "\n",
    "## defining the word2vec model\n",
    "model_w2v = gensim.models.Word2Vec(tokenized_tweet,vector_size=200,window=5,\n",
    "            min_count=2,sg = 1,hs = 0,negative = 10,workers= 32\n",
    ") \n",
    "\n",
    "## training the word2vec model\n",
    "model_w2v.train(tokenized_tweet, total_examples= len(data_copy['clean_tweet']), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "C53O5x3zsZsk",
    "outputId": "56af89a0-aad5-4094-e83e-6eb5e8440551"
   },
   "outputs": [],
   "source": [
    "def find_vector(tokens, size):\n",
    "    vector_ = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vector_ += model_w2v.wv[word].reshape((1, size)) ## vector for the word 1x200\n",
    "            count += 1.\n",
    "        except KeyError:  # token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0: ## normalizing the vector\n",
    "        vector_ /= count\n",
    "    return vector_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "y7ar1pxvl1fx"
   },
   "outputs": [],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "### forming the dataframe of vectors representing the tweets\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = find_vector(tokenized_tweet.iloc[i], 200)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape\n",
    "x_train,x_val,y_train,y_val = train_test_split(wordvec_df,y,shuffle=True,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.12934613227844238\n",
      "Logistic regression Accuracy and F1 Score:\n",
      "0.56\n",
      "0.6206896551724138\n",
      "time taken: 0.37081217765808105\n",
      "SVM Accuracy and F1 Score:\n",
      "0.57\n",
      "0.6416666666666667\n",
      "time taken: 0.46596598625183105\n",
      "Random Forrest Accuracy and F1 Score:\n",
      "0.565\n",
      "0.6614785992217899\n"
     ]
    }
   ],
   "source": [
    "classify(x_train,x_val,y_train,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJv5xjJ4gE7D"
   },
   "source": [
    "### Doc2vec Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4eWoA6RPRHD",
    "outputId": "319e6098-2383-4e33-ed06-50930b024b24"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"progress-bar\") \n",
    "# label all the tweets\n",
    "def add_label(tweet):\n",
    "    output = []\n",
    "    for i, s in zip(tweet.index, tweet):\n",
    "        output.append(TaggedDocument(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "\n",
    "labeled_tweets = add_label(tokenized_tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WI1_3gc7nC1G",
    "outputId": "abe8d965-2a71-4983-f75e-59ae558c668b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2820648.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.06 s, sys: 349 ms, total: 4.41 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### defining model paramters\n",
    "model_d2v = gensim.models.Doc2Vec(dm=1,dm_mean=1,vector_size=200,window=5, \n",
    "                                  negative=7,min_count=5,workers=32,alpha=0.1)\n",
    "\n",
    "## forming vocabulary of words\n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n",
    "\n",
    "## training the doc2vec model\n",
    "model_d2v.train(labeled_tweets, total_examples= len(data_copy['clean_tweet']), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNoF1fxDnF5w",
    "outputId": "24f42106-8ac9-430e-d493-d42c516c739b"
   },
   "outputs": [],
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "\n",
    "### forming the dataframe of vectors representing the tweets\n",
    "for i in range(len(x)):\n",
    "    docvec_arrays[i,:] = model_d2v.dv[i].reshape((1,200))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "docvec_df.shape\n",
    "x_train,x_val,y_train,y_val = train_test_split(docvec_df,y,shuffle=True,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.045105934143066406\n",
      "Logistic regression Accuracy and F1 Score:\n",
      "0.635\n",
      "0.6919831223628692\n",
      "time taken: 0.30423951148986816\n",
      "SVM Accuracy and F1 Score:\n",
      "0.585\n",
      "0.6914498141263941\n",
      "time taken: 0.481886625289917\n",
      "Random Forrest Accuracy and F1 Score:\n",
      "0.545\n",
      "0.6539923954372623\n"
     ]
    }
   ],
   "source": [
    "classify(x_train,x_val,y_train,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZVOno87gKIo"
   },
   "source": [
    "### Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HcXBah_4JRFN"
   },
   "outputs": [],
   "source": [
    "def bert_preprocessing(text):\n",
    "    #Removing Twitter Handles\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Function to tokenize a set of texts\n",
    "def bert_tokens(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for d_ in data:\n",
    "        encoded_data = tokenizer.encode_plus(\n",
    "            text=bert_preprocessing(d_),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,            \n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_data.get('input_ids'))\n",
    "        attention_masks.append(encoded_data.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  82\n",
      "time taken: 0.3108561038970947\n"
     ]
    }
   ],
   "source": [
    "# Encoding the tweets\n",
    "start = time.time()\n",
    "encoded_tweets = [tokenizer.encode(data, add_special_tokens=True) for data in data_copy.tweet]\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)\n",
    "stop = time.time()\n",
    "print('time taken:',stop-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val =train_test_split(x, y, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  graduation wedding season work week runs thru weekends\n",
      "Token ids:  [101, 7665, 5030, 2161, 2147, 2733, 3216, 27046, 13499, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = max_len\n",
    "token_ids = list(bert_tokens([x[0]])[0].squeeze().numpy())\n",
    "print('Tweet: ', x[0])\n",
    "print('Token ids: ', token_ids)\n",
    "\n",
    "## tokenizing data\n",
    "train_inputs, train_masks = bert_tokens(X_train)\n",
    "val_inputs, val_masks = bert_tokens(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 0.08398175239562988\n",
      "Logistic regression Accuracy and F1 Score:\n",
      "0.45\n",
      "0.5925925925925926\n",
      "time taken: 0.22464585304260254\n",
      "SVM Accuracy and F1 Score:\n",
      "0.44\n",
      "0.6111111111111112\n",
      "time taken: 0.16375088691711426\n",
      "Random Forrest Accuracy and F1 Score:\n",
      "0.46\n",
      "0.5781249999999999\n"
     ]
    }
   ],
   "source": [
    "## converting tensors to numpy for classification with ML algorithms\n",
    "x_train_n = train_inputs.cpu().detach().numpy()\n",
    "x_val_n = val_inputs.cpu().detach().numpy()\n",
    "\n",
    "classify(x_train_n,x_val_n,y_train,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuned Bert Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to Tensors\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Training data dataloader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Validation data dataloader\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 µs, sys: 5 µs, total: 53 µs\n",
      "Wall time: 57.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 50, 2 # Hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        \n",
    "        ## using pretrained bert model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        \n",
    "        # defining classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        outputs = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to initialize bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(epochs=4):\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "    bert_classifier.to(device)\n",
    "    \n",
    "    optimizer = AdamW(bert_classifier.parameters(),lr=5e-5,eps=1e-8  )\n",
    "\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "def set_seed(seed_value=4):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    print(\"Training started...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradients to 1.0 to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values after 50 batches, you can change this value\n",
    "            if (step % 200 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        if evaluation == True:\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            __, train_accuracy = evaluate(model, train_dataloader)\n",
    "\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "            print(\"Train Accuracy:\",train_accuracy)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the bert classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   28    |   0.677812   |     -      |     -     |   13.35  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.677812   |  0.682677  |   55.47   |   17.06  \n",
      "----------------------------------------------------------------------\n",
      "Train Accuracy: 76.29310344827586\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   28    |   0.535442   |     -      |     -     |   10.03  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.535442   |  0.741207  |   58.59   |   13.75  \n",
      "----------------------------------------------------------------------\n",
      "Train Accuracy: 87.06896551724138\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   28    |   0.321859   |     -      |     -     |   10.04  \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.321859   |  1.046823  |   56.25   |   13.77  \n",
      "----------------------------------------------------------------------\n",
      "Train Accuracy: 95.58189655172414\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   28    |   0.164185   |     -      |     -     |   10.04  \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.164185   |  1.304907  |   56.25   |   13.77  \n",
      "----------------------------------------------------------------------\n",
      "Train Accuracy: 98.0603448275862\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   28    |   0.079998   |     -      |     -     |   10.04  \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.079998   |  1.479599  |   57.03   |   13.77  \n",
      "----------------------------------------------------------------------\n",
      "Train Accuracy: 98.92241379310344\n",
      "\n",
      "\n",
      "Training complete!\n",
      "CPU times: user 1min, sys: 17.5 s, total: 1min 18s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed(4)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = create_model(epochs=5)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction function for the BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_fun(model, test_dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating accuracy and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy and F1 score of bert classifier:\n",
      "0.66\n",
      "0.6458333333333334\n"
     ]
    }
   ],
   "source": [
    "probs = pred_fun(bert_classifier, val_dataloader)\n",
    "preds = probs[:,1] >= 0.4 # if prediction is greater than or equal to 0.4 than 1 else 0 \n",
    "preds = preds.astype(int)\n",
    "# Evaluate the Bert classifier\n",
    "print('Accuracy and F1 score of bert classifier:')\n",
    "print(accuracy_score(preds, y_val))\n",
    "print(f1_score(preds, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hpml_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
